from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
import json
import os

class EmbeddingEncoder:
    def __init__(self, model, tokenizer_name=None, device=None, adapter_name=None):
        token = os.getenv('HF_TOKEN')
        """
        Initializes the EmbeddingEncoder with a specified model or model name, an optional tokenizer, and sets the device for computation.
        
        Args:
            model (Union[str, torch.nn.Module]): The model identifier from Hugging Face's model hub or a model instance.
            tokenizer_name (str): Optional; the tokenizer identifier. If None, uses model if it's a string.
            device (str): Optional; the device to run the model on ('cuda' or 'cpu'). If None, uses cuda if available.
            adapter_name (str): Optional; the adapter identifier to load with the model, either a path or from the Hugging Face hub.
        """
        if tokenizer_name is None:
            tokenizer_name = model if isinstance(model, str) else None
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        if isinstance(model, str):
            self.model = AutoModel.from_pretrained(model, token=token).to(self.device)
            if adapter_name:
                # Check if the adapter path exists or it is from the hub
                if os.path.exists(adapter_name):
                    # Load adapter from the local path
                    self.model.load_adapter(adapter_name)
                else:
                    # Load adapter from Hugging Face Hub
                    self.model.load_adapter(adapter_name, source='hf')
        else:
            self.model = model.to(self.device)

        if tokenizer_name:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.model.eval()  # Set the model to evaluation mode

    def encode(self, text_data, output_path=None, return_format='numpy', text_column=None):
        """
        Encodes the provided text data into embeddings using the initialized model.

        Args:
            text_data (str, list, or dict): Text data to encode. Can be a single string, list of strings, or a JSON-like dictionary.
            output_path (str): Optional path to save the output (json or numpy file).
            return_format (str): The format of the return value ('numpy', 'json', or 'list').
            text_column (str): Optional; key to use if text_data is a dictionary (for JSON-like structures).

        Returns:
            Embeddings as specified by return_format.
        """
        # Handling different types of text data input
        if isinstance(text_data, dict):
            if text_column is None:
                raise ValueError("text_column must be specified if text_data is a dictionary")
            text_data = [item[text_column] for item in text_data]
        elif isinstance(text_data, str):
            text_data = [text_data]  # Convert a single string to a list

        # Prepare the text data for the model
        inputs = self.tokenizer(text_data, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)

        # Generate embeddings
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state[:, 0, :]  # Taking the CLS token representation

        # Handling different return formats
        if return_format == 'json':
            embeddings = embeddings.cpu().numpy()  # Convert to CPU and numpy only if necessary
            formatted_data = [{'text': text, 'embedding': emb.tolist()} for text, emb in zip(text_data, embeddings)]
            if output_path:
                with open(output_path, 'w') as f:
                    json.dump(formatted_data, f)
            return formatted_data
        elif return_format == 'list':
            return embeddings.cpu().tolist()  # Convert to CPU and list only if necessary
        else:
            if output_path:
                embeddings_np = embeddings.cpu().numpy()  # Convert to CPU and numpy only if necessary
                np.save(output_path, embeddings_np)
            return embeddings
    
    def format(self, embeddings, text_data, output_path=None):
        """
        Organizes embeddings along with their corresponding text data into a structured JSON format.

        Args:
            embeddings (numpy.ndarray or list): List of embeddings, typically generated by the `encode()` method.
            text_data (list): List of text data corresponding to each embedding.
            output_path (str): Optional; path to save the formatted JSON output.

        Returns:
            None; outputs a JSON file at the specified `output_path` containing the embeddings and text data, or
            returns the formatted dictionary if `output_path` is None.
        """
        # Ensure embeddings are in a list format for JSON serialization
        if isinstance(embeddings, np.ndarray):
            embeddings = embeddings.tolist()

        # Combine text data and embeddings into a structured dictionary
        formatted_data = [{'text': text, 'embedding': emb} for text, emb in zip(text_data, embeddings)]

        # If an output path is provided, write the data to a JSON file
        if output_path:
            with open(output_path, 'w') as f:
                json.dump(formatted_data, f)
        else:
            return formatted_data



